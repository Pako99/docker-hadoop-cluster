# ██████╗  ██████╗ █████╗
# ██╔══██╗██╔════╝██╔══██╗
# ██║  ██║██║     ███████║
# ██║  ██║██║     ██╔══██║
# ██████╔╝╚██████╗██║  ██║
# ╚═════╝  ╚═════╝╚═╝  ╚═╝
# DEPARTAMENTO DE ENGENHARIA DE COMPUTACAO E AUTOMACAO
# UNIVERSIDADE FEDERAL DO RIO GRANDE DO NORTE, NATAL/RN
#
# (C) 2023 CARLOS M D VIEGAS
# https://github.com/cmdviegas


### Description:
## This is a docker-compose file that creates a stack of nodes of Apache Hadoop 3.3.5 with Spark 3.4.0,
## optionally, it includes Apache Hive 3.1.3 with Postgresql 15.2 as metastore.
#
### How it works:
## It initializes 'node-master' service and 'node-X' as worker nodes (according to the number of replicas).
## 'node-master' starts hadoop and spark services and then creates a cluster by connecting to each 'node-X'.
## There is an .env file that defines some environment variables that should be edited by the user.
## If 'hiveexternal' variable is set to 'true', it also initilizes 'db' service with postgresql as metastore 
## for hive. For this, it is mandatory do include '--profile hiveexternal' when starting 'db' service.
#

version: '3.9'

services:
  node-master:
    container_name: node-master
    hostname: node-master
    image: hadoopcluster/${IMAGE_VER}
    build:
      context: .
      dockerfile: Dockerfile
      args:
        USER: ${SYS_USERNAME}
        PASS: ${SYS_PASSWORD}
        PGUSER: ${PSQL_PGUSER}
        PGPASSWORD: ${PSQL_PGPASSWORD}
        HIVEEXTERNAL: ${HIVE_EXTERNAL}
        REPLICAS: ${NODE_REPLICAS}
    tty: true
    restart: unless-stopped
    networks:
      hadoop_network:
        ipv4_address: 172.18.0.2
    ports:
      - "9870:9870"
      - "8088:8088"
      - "18080:18080"
      - "2222:22"
    depends_on:
      db:
        condition: service_healthy
    volumes: 
      - ./apps:/home/${SYS_USERNAME}/apps
      - vol-data-master:/home/${SYS_USERNAME}/
    healthcheck:
      test: bash -c 'ssh -q -o ConnectTimeout=2 ${SYS_USERNAME}@172.18.0.2 exit'
      start_period: 3s
      interval: 2s
      timeout: 3s
      retries: 3

  node:
    image: hadoopcluster/${IMAGE_VER}
    deploy:
      mode: replicated
      replicas: ${NODE_REPLICAS}
    tty: true
    restart: unless-stopped
    networks:
      - hadoop_network
    depends_on:
      node-master:
        condition: service_healthy
    volumes: 
      - ./apps:/home/${SYS_USERNAME}/apps
      - /home/${SYS_USERNAME}/

  db:
    container_name: db
    hostname: postgres-db
    image: postgres:15.2
    tty: true
    restart: unless-stopped
    environment:
      - PGUSER=${PSQL_PGUSER}
      - PGPASSWORD=${PSQL_PGPASSWORD}
      - PGDATA=/postgres/data
      - POSTGRES_HOST_AUTH_METHOD=md5
      - POSTGRES_DB=${PSQL_DBNAME}
      - POSTGRES_USER=${PSQL_PGUSER}
      - POSTGRES_PASSWORD=${PSQL_PGPASSWORD}
    profiles:
      - hiveexternal
    expose:
      - "5432:5432"
    networks:
      hadoop_network:
        ipv4_address: 172.18.0.20
    volumes: 
      - ./apps:/postgres/apps
      - postgres-db:/postgres/data
      - ./sql/postgresql.conf:/postgres/conf/postgresql.conf
      - ./sql/pg_hba.conf:/postgres/conf/pg_hba.conf
      - ./sql/set_sql_permission.sh:/docker-entrypoint-initdb.d/set_sql_permission.sh
    command: postgres -c config_file=/postgres/conf/postgresql.conf
    healthcheck:
      test: ["CMD-SHELL", "pg_isready"]
      start_period: 3s
      interval: 1s
      timeout: 3s
      retries: 2

networks:
  hadoop_network:
    driver: bridge
    ipam:
      driver: default
      config:
        - subnet: 172.18.0.0/24

volumes:
  vol-data-master:
  postgres-db:
